{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "import whisper\n",
    "import re\n",
    "from whisper.utils import format_timestamp\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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' # os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transcript(whisper_out):\n",
    "        \n",
    "    transcript_str = []\n",
    "    transcript = []\n",
    "    for segment in whisper_out['segments']:\n",
    "        transcript_str.append(f\"[{format_timestamp(segment['start'])}]:\\t{segment['text']}\")\n",
    "        transcript.append(\n",
    "            {\n",
    "                'time': segment['start'],\n",
    "                'timestamp': format_timestamp(segment['start']),\n",
    "                'text': segment['text']\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    transcript_str = \"\\n\".join(transcript_str)\n",
    "    return transcript_str, transcript\n",
    "\n",
    "def postprocess_points(raw_output):\n",
    "    raw_output = re.sub(r'\\n\\s*-', '\\n-', raw_output)\n",
    "    points = raw_output.split('\\n-')\n",
    "    points = [point.strip() for point in points]\n",
    "    points = [point for point in points if point != '']\n",
    "    return points\n",
    "\n",
    "def find_closest_time(time, all_times):\n",
    "    closest_time = min(all_times, key=lambda x: abs(x - time))\n",
    "    return closest_time\n",
    "\n",
    "def create_transcript_chunks(transcript, stride=3, length=3):\n",
    "    '''Create larger chunks of the segments using a sliding window'''\n",
    "    all_start_times = [segment['time'] for segment in transcript]\n",
    "\n",
    "    transcript_chunks = []\n",
    "    for i in range(0, len(all_start_times), stride):\n",
    "        chunk = {}\n",
    "\n",
    "        chunk['time'] = all_start_times[i]\n",
    "\n",
    "        chunk['text'] = \"\".join([segment['text'] for segment in transcript[i:i+length]]).strip()\n",
    "\n",
    "        transcript_chunks.append(chunk)\n",
    "    \n",
    "    return transcript_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LISAPipeline():\n",
    "    def __init__(self, whisper_model, search_model):\n",
    "        print('Loading Whisper...')\n",
    "        self.whisper_model = whisper.load_model(whisper_model)\n",
    "        print('Loading Sentence Transformer...')\n",
    "        self.search_model = SentenceTransformer.load(search_model)\n",
    "        print('Models loaded!')\n",
    "    \n",
    "    def run_gpt3(self, prompt, max_tokens=256, temperature=0.5, top_p=1, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty\n",
    "        )\n",
    "        return response.choices[0].text\n",
    "    \n",
    "    def transcribe(self, audio_path):\n",
    "        whisper_out = self.whisper_model.transcribe(audio_path, verbose=False)\n",
    "        return whisper_out\n",
    "    \n",
    "    def minutes_of_meeting(self, transcript_str):\n",
    "\n",
    "        mom_prompt = f\"\"\"Generate a meeting summary/notes for the following transcript:\n",
    "        Meeting Transcription:\n",
    "        {transcript_str}\n",
    "\n",
    "        Instructions:\n",
    "        1. Do not use the same words as in the transcript.\n",
    "        2. Use proper grammar and punctuation.\n",
    "        3. Use bullets to list the points.\n",
    "        4. Add as much detail as possible.\n",
    "\n",
    "        Meeting Minutes:\n",
    "        -\"\"\"\n",
    "\n",
    "        raw_minutes = self.run_gpt3(mom_prompt, temperature=0.5)\n",
    "        minutes = postprocess_points(raw_minutes)\n",
    "\n",
    "        return minutes\n",
    "\n",
    "    def action_items(self, transcript_str):\n",
    "\n",
    "        action_prompt = f\"\"\"Extract the Action Items / To-Do List from the Transcript.\n",
    "        Meeting Transcription:\n",
    "        {transcript_str}\n",
    "\n",
    "        Action Items:\n",
    "        -\"\"\"\n",
    "        raw_action_items = self.run_gpt3(action_prompt, temperature=0.4)\n",
    "        action_items = postprocess_points(raw_action_items)\n",
    "\n",
    "        return action_items\n",
    "\n",
    "    def create_index(self, whisper_out, transcript):\n",
    "        '''Create search index by embedding the transcript segments'''\n",
    "        all_start_times = [segment['start'] for segment in whisper_out['segments']]\n",
    "        all_end_times = [segment['end'] for segment in whisper_out['segments']]\n",
    "\n",
    "        transcript_chunks = create_transcript_chunks(all_start_times, all_end_times, transcript, stride=45, length=60)\n",
    "\n",
    "        # Encode query and documents\n",
    "        chunk_texts = [chunk['text'] for chunk in transcript_chunks]\n",
    "        doc_emb = self.search_model.encode(chunk_texts)\n",
    "\n",
    "        return doc_emb, transcript_chunks\n",
    "    \n",
    "    def search(self, doc_embeddings, transcript_chunks, query, top_k=3, threshold=14):\n",
    "        # Compute dot score between query and all document embeddings\n",
    "        query_embeddings = self.search_model.encode(query)\n",
    "        scores = util.dot_score(query_embeddings, doc_embeddings)[0].cpu().tolist()\n",
    "\n",
    "        chunks = [(chunk['start'], chunk['text']) for chunk in transcript_chunks]\n",
    "\n",
    "        # Combine docs & scores\n",
    "        chunk_score_tuples = [(*chunks[i], scores[i]) for i in range(len(chunks))]\n",
    "\n",
    "        # Sort by decreasing score\n",
    "        chunk_score_tuples = sorted(chunk_score_tuples, key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "        # Output passages & scores\n",
    "        results = []\n",
    "        for start, text, score in chunk_score_tuples[:top_k]:\n",
    "            results.append({\n",
    "                'time': start,\n",
    "                'timestamp': format_timestamp(start),\n",
    "                'text': text,\n",
    "                'confidence': score\n",
    "            })\n",
    "            # if score > threshold:\n",
    "            #     results.append((start, end, text))\n",
    "            # print('Score', score, text)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __call__(self, audio_path):\n",
    "        '''Run the pipeline on an audio file'''\n",
    "        whisper_out = self.transcribe(audio_path)\n",
    "        transcript_str, transcript = create_transcript(whisper_out)\n",
    "        minutes = self.minutes_of_meeting(transcript_str)\n",
    "        action_items = self.action_items(transcript_str)\n",
    "        doc_emb, transcript_chunks = self.create_index(whisper_out, transcript)\n",
    "\n",
    "        return minutes, action_items, doc_emb, transcript_chunks, transcript_str, transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper...\n",
      "Loading Sentence Transformer...\n",
      "Models loaded!\n"
     ]
    }
   ],
   "source": [
    "lisa = LISAPipeline(whisper_model=\"../whisper_models/medium.pt\", search_model=\"../multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19355/19355 [00:45<00:00, 427.42frames/s]\n"
     ]
    }
   ],
   "source": [
    "minutes, action_items, doc_emb, transcript_chunks, transcript_str, transcript = lisa('Sample Meeting.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transcript_chunks(transcript, stride=3, length=3):\n",
    "    '''Create larger chunks of the segments using a sliding window'''\n",
    "    all_start_times = [segment['time'] for segment in transcript]\n",
    "\n",
    "    transcript_chunks = []\n",
    "    for i in range(0, len(all_start_times), stride):\n",
    "        chunk = {}\n",
    "\n",
    "        chunk['time'] = all_start_times[i]\n",
    "\n",
    "        chunk['text'] = \"\".join([segment['text'] for segment in transcript[i:i+length]]).strip()\n",
    "\n",
    "        transcript_chunks.append(chunk)\n",
    "    \n",
    "    return transcript_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'time': 0.0,\n",
       "  'timestamp': '00:00.000',\n",
       "  'text': \" so yeah so let's get started so let me just share my screen okay so I did the\"},\n",
       " {'time': 24.22,\n",
       "  'timestamp': '00:24.220',\n",
       "  'text': ' toy neurosymbolic thing so I created a sample neurosymbolic program synthesis'},\n",
       " {'time': 31.439999999999998,\n",
       "  'timestamp': '00:31.440',\n",
       "  'text': ' engine so I used GPT-3 for that so there were a few observations that I made but'},\n",
       " {'time': 37.08,\n",
       "  'timestamp': '00:37.080',\n",
       "  'text': ' before that let me tell you what I did so I basically created a few prompt'},\n",
       " {'time': 43.879999999999995,\n",
       "  'timestamp': '00:43.880',\n",
       "  'text': ' example where I put in all the models so for each model I had a description I had'},\n",
       " {'time': 50.72,\n",
       "  'timestamp': '00:50.720',\n",
       "  'text': ' the arguments and the data types and the return what it returns and I also had a'},\n",
       " {'time': 59.08,\n",
       "  'timestamp': '00:59.080',\n",
       "  'text': ' few problem statements couple problem statement as examples with their'},\n",
       " {'time': 62.64,\n",
       "  'timestamp': '01:02.640',\n",
       "  'text': ' solution workflows so that was my prompting so a few observations that I'},\n",
       " {'time': 67.08,\n",
       "  'timestamp': '01:07.080',\n",
       "  'text': ' made were prompting with examples of feedback and corrections made the final'},\n",
       " {'time': 71.52,\n",
       "  'timestamp': '01:11.520',\n",
       "  'text': ' up would be more correct in one shot without requiring further changes so I'},\n",
       " {'time': 75.12,\n",
       "  'timestamp': '01:15.120',\n",
       "  'text': ' also did a feedback loop kind of situation where I used another instance'},\n",
       " {'time': 80.82000000000001,\n",
       "  'timestamp': '01:20.820',\n",
       "  'text': ' of GPT-3 which would give feedback on what could be changed what would be'},\n",
       " {'time': 84.48,\n",
       "  'timestamp': '01:24.480',\n",
       "  'text': ' improved what are the issues and that sort of thing and I also yeah so that'},\n",
       " {'time': 93.04,\n",
       "  'timestamp': '01:33.040',\n",
       "  'text': ' kind of so although I would recommend that so my suggestion is to or like my'},\n",
       " {'time': 99.4,\n",
       "  'timestamp': '01:39.400',\n",
       "  'text': ' approach or my cost-effective approach would be to rather fine-tune a critic'},\n",
       " {'time': 104.2,\n",
       "  'timestamp': '01:44.200',\n",
       "  'text': ' feedback model and use that for critic because critic is would be the most'},\n",
       " {'time': 109.56,\n",
       "  'timestamp': '01:49.560',\n",
       "  'text': ' important part because if that fails so we cannot rely on a few short prompted'},\n",
       " {'time': 113.48,\n",
       "  'timestamp': '01:53.480',\n",
       "  'text': ' GPT-3 critic instead I would put my bets on a fine-tuned version and also based'},\n",
       " {'time': 121.36,\n",
       "  'timestamp': '02:01.360',\n",
       "  'text': ' on the paper that you gave me that is the memory assisted prompt editing to'},\n",
       " {'time': 124.92,\n",
       "  'timestamp': '02:04.920',\n",
       "  'text': ' improve GPT-3 so what we could do is we could maintain a memory of all problem'},\n",
       " {'time': 130.52,\n",
       "  'timestamp': '02:10.520',\n",
       "  'text': ' statements and use document-based similarity matching to retrieve similar'},\n",
       " {'time': 136.88000000000002,\n",
       "  'timestamp': '02:16.880',\n",
       "  'text': ' problem statements and get those additional feedbacks and all that have'},\n",
       " {'time': 140.36,\n",
       "  'timestamp': '02:20.360',\n",
       "  'text': ' been given to those workflow outputs so this gives the model more context and'},\n",
       " {'time': 145.44,\n",
       "  'timestamp': '02:25.440',\n",
       "  'text': ' creates mechanism to use its knowledge from memory and improve efficiency'},\n",
       " {'time': 149.0,\n",
       "  'timestamp': '02:29.000',\n",
       "  'text': ' selectively so the fine so the items at hand or the action items that we have at'},\n",
       " {'time': 157.48,\n",
       "  'timestamp': '02:37.480',\n",
       "  'text': ' our hand would be yeah so the next step that we could do is we could work on'},\n",
       " {'time': 166.6,\n",
       "  'timestamp': '02:46.600',\n",
       "  'text': ' this memory bank solution for the program synthesis part the next would be'},\n",
       " {'time': 172.72,\n",
       "  'timestamp': '02:52.720',\n",
       "  'text': ' creating a model catalog the next task would be creating a logical planner and'},\n",
       " {'time': 178.64,\n",
       "  'timestamp': '02:58.640',\n",
       "  'text': ' the other task would be creating a pipeline executor for the same'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'time': 0.0,\n",
       "  'text': \"so yeah so let's get started so let me just share my screen okay so I did the toy neurosymbolic thing so I created a sample neurosymbolic program synthesis\"},\n",
       " {'time': 31.439999999999998,\n",
       "  'text': 'engine so I used GPT-3 for that so there were a few observations that I made but before that let me tell you what I did so I basically created a few prompt'},\n",
       " {'time': 43.879999999999995,\n",
       "  'text': 'example where I put in all the models so for each model I had a description I had the arguments and the data types and the return what it returns and I also had a'},\n",
       " {'time': 59.08,\n",
       "  'text': 'few problem statements couple problem statement as examples with their solution workflows so that was my prompting so a few observations that I'},\n",
       " {'time': 67.08,\n",
       "  'text': 'made were prompting with examples of feedback and corrections made the final up would be more correct in one shot without requiring further changes so I'},\n",
       " {'time': 75.12,\n",
       "  'text': 'also did a feedback loop kind of situation where I used another instance of GPT-3 which would give feedback on what could be changed what would be'},\n",
       " {'time': 84.48,\n",
       "  'text': 'improved what are the issues and that sort of thing and I also yeah so that kind of so although I would recommend that so my suggestion is to or like my'},\n",
       " {'time': 99.4,\n",
       "  'text': 'approach or my cost-effective approach would be to rather fine-tune a critic feedback model and use that for critic because critic is would be the most'},\n",
       " {'time': 109.56,\n",
       "  'text': 'important part because if that fails so we cannot rely on a few short prompted GPT-3 critic instead I would put my bets on a fine-tuned version and also based'},\n",
       " {'time': 121.36,\n",
       "  'text': 'on the paper that you gave me that is the memory assisted prompt editing to improve GPT-3 so what we could do is we could maintain a memory of all problem'},\n",
       " {'time': 130.52,\n",
       "  'text': 'statements and use document-based similarity matching to retrieve similar problem statements and get those additional feedbacks and all that have'},\n",
       " {'time': 140.36,\n",
       "  'text': 'been given to those workflow outputs so this gives the model more context and creates mechanism to use its knowledge from memory and improve efficiency'},\n",
       " {'time': 149.0,\n",
       "  'text': 'selectively so the fine so the items at hand or the action items that we have at our hand would be yeah so the next step that we could do is we could work on'},\n",
       " {'time': 166.6,\n",
       "  'text': 'this memory bank solution for the program synthesis part the next would be creating a model catalog the next task would be creating a logical planner and'},\n",
       " {'time': 178.64,\n",
       "  'text': 'the other task would be creating a pipeline executor for the same'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "create_transcript_chunks(transcript, stride=2, length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "np.array(doc_emb.tolist()) == doc_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_out = lisa.transcribe('Recording.m4a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_str = create_transcript_str(whisper_out)\n",
    "minutes = lisa.minutes_of_meeting(transcript_str)\n",
    "action_items = lisa.action_items(transcript_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb, transcript_chunks = lisa.create_index(whisper_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bhavish introduced himself.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lisa.search(doc_emb, transcript_chunks, 'explain the issue with deep learning with an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = mp.VideoFileClip('Recording.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.audio.write_audiofile(\"Recording2.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c34959fd061e0b75fe61b25789565ee0f65e3017c38ac9c89fb2c0e5483aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
