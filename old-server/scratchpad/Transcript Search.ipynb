{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from whisper.utils import format_timestamp\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer.load('multi-qa-mpnet-base-dot-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript = \"\"\"\n",
    "# [00:00:00]:\tWithout further ado, let’s bring out the Steve I think you all are here to see.\n",
    "\n",
    "# [00:00:10]: \tSteve Jobs.\n",
    "\n",
    "# [00:00:24]:\tJust this week you surpassed Microsoft in, um, market valuation, and we thought you might have a thought or two about that.\n",
    "\n",
    "# [00:00:35]: \tUm… it… for those of us that have been in the industry a long time, it’s surreal. Uh… but, it… it doesn’t matter very much.\n",
    "\n",
    "# [00:00:45]:\tSo, you published a long, open letter. Something like, what did you call it? Thoughts on Flash or something like that. So, you’ve… you’ve clearly, uh, stated your case there. Is it really fair or the best thing for consumers who buy… say an iPad or an iPhone to just be abrupt?\n",
    "\n",
    "# [00:01:05]:\tApple is a company that has… doesn’t have the most resources of everybody in the world. And, um, the way we’ve succeeded is by choosing what horses to ride really carefully, technically. We try to look for these technical vectors that have a future. And that are headed up. And, you know, technology… different pieces of technology kind of go in cycles. They have their, their springs and summers and autumns and then they, you know, go to the graveyard of technology. And so we try to pick things that are in their springs. And if you choose wisely, you can save yourself an enormous amount of work versus trying to do everything. So, we have a history of doing that. As an example, we went from the five inch floppy disk to three and a half inch floppy disk with the Mac. And sometimes when we get rid of things, like the [00:02:00] floppy disk drive on the original iMac, people call us crazy. But, sometimes you just have to pick the things that look like they’re going to be the right horses to ride going forward. And… and flash looks like a technology that had its day. But is really on the… is waning. And HTML 5 looks like the technology that’s really on the ascendancy right now.\n",
    "\n",
    "# [00:02:20]:\tIt’s also a development environment and there are entire websites… some of them quite beautiful… written on a flash platform.\n",
    "\n",
    "# [00:02:28]:\tYou know, one of the… an even more popular development environment was HyperCard, and we were okay to axe that. It wasn’t more popular than Flash, was it?\n",
    "\n",
    "# [00:02:38]:\tOh… in its day, sure it was.\n",
    "\n",
    "# [00:02:40]:\tOn your platform, right?\n",
    "\n",
    "# [00:04:35]: Hey Lisa, add a joke about the iPad being slow in the action items.\n",
    "\n",
    "# [00:04:45]: Also add reinstalling flash to the list.\n",
    "\n",
    "# [00:02:43]:\tNo, no, no, no, no. HyperCard was huge in its day because it was… because it was accessible to anybody. We… we didn’t start off to have a war with Flash or anything else. We just made a technical decision that we weren’t going to put the energy into getting Flash on our platform. We told Adobe, “If you ever have this thing running fast, come back and show us.” Which they never did. And… uh… but… but people were not going to use it. And that was it. And we shipped the iPhone. And it doesn’t use Flash. And it wasn’t until we shipped the iPad, and it didn’t use flash, that Adobe started to raise a stink about it. And that’s why I wrote Thoughts on Flash… was because we were trying to be real professional about this and weren’t talking to the press about it. We didn’t think it was a matter for the press. And we finally just said, enough is enough. We’re tired of these guys trashing us in the press over this.\n",
    "\n",
    "# [00:03:15]:\tWhat if the market says, “Hey, you know, it’s important enough to us to be able to run not just these videos but whole websites.” What if people say, you know, the iPad is… uh… crippled in this respect?\n",
    "\n",
    "# [00:03:25]:\tWell, you know, well I’d say two things. Number one, things are packages of… of emphasis. Some things are emphasized in a product. Some things are not done as well in a product. Some things are chosen not to be done at all in a product. And so different people make [00:04:00] different choices. And, uh, if the market tells we’re making the wrong choices, we listen to the market. We’re just… we’re just people running this company. We’re trying to make great products for people. And so we have at least the courage of our convictions to say we don’t think this is part of what makes a great product. We’re going to leave it out. That’s what a lot of customers pay us to do… is to try to make the products that we can. And if we succeed, they’ll buy them. And if we don’t, they won’t. And it’ll all work itself out.\n",
    "\n",
    "# [00:04:10]:\tSo, uh.\n",
    "\n",
    "# [00:04:12]: \tSo, you know so far, I’d have to say, that that that people seem to be liking iPads. You know. I mean we’ve sold one every three seconds since launched it. So, I don’t know how it’s going to come out.\n",
    "# 2\n",
    "\n",
    "# [00:04:30]:\tIt takes you three whole seconds to sell one? Can’t you do better?\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.json') as f:\n",
    "    data = json.load(f)\n",
    "    full_text = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transcript\n",
    "transcript = []\n",
    "for segment in data['segments']:\n",
    "    transcript.append((format_timestamp(segment['start']), segment['text'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000]:\tSo, hello everybody. Welcome to the final module of this course on Neuro-Symbolic AI.\n",
      "[00:11.240]:\tSo in the past few, like in the past, we have like, in the previous session, we looked at\n",
      "[00:20.400]:\tsymbolic AI and we looked at basically, propositional logic, first order logic and program synthesis.\n",
      "[00:26.260]:\tSo the program synthesis part is going to be something that we are going to use today.\n",
      "[00:30.760]:\tAnd we also looked at relation networks last time, although it was just like an overview\n",
      "[00:35.520]:\tof that. So today we will conclude by learning about neuro-symbolic AI. So let's get started\n",
      "[00:46.800]:\there.\n",
      "[00:47.800]:\tOkay. So the contents for this session are going to be basically, we'll start of course\n",
      "[01:10.760]:\tby talking about the problem. We'll again have a very quick review of symbolic AI, although\n",
      "[01:16.120]:\twe just looked at, but just a very quick review of symbolic AI from last time. And we also\n",
      "[01:22.360]:\thave like a difference between neural networks and symbolic AI. And then we look at the sort\n",
      "[01:29.320]:\tof flavor data set again, but a bit of a modified version that I created for neuro-symbolic\n",
      "[01:34.040]:\tAI. So we'll look at that and then we move on to the actual architecture and all the\n",
      "[01:39.400]:\tmodules and everything that we'll be utilizing. So the entire work will start from here.\n",
      "[01:45.120]:\tSo I guess we had already looked at this particular problem a while back, I think in one of the\n",
      "[01:55.080]:\tfirst few modules, I guess. So basically what we have here is basically an image of a flood,\n",
      "[02:01.960]:\tright? So if we give a deep learning model, which was not trained on floods or anything,\n",
      "[02:06.840]:\tit would obviously, a deep learning model would only like predict a few labels, I guess.\n",
      "[02:13.200]:\tSo maybe like water and person. So it might say there's water, okay. So the deep learning\n",
      "[02:17.680]:\tmodel might just say water and there are people in it, so it will either say people or person.\n",
      "[02:25.320]:\tBut apart from that, if we humans are given a definition of a flood, then we will actually\n",
      "[02:30.880]:\tbe able to reason about an image and get the flood answer, right? So as a human, what we\n",
      "[02:37.520]:\tdo, we also see both water and people there, right? But we also see that there's water\n",
      "[02:48.200]:\tperson, but also we see that it's not really a beach or anything, right? It's not like\n",
      "[02:52.600]:\tbecause even in a beach, people are there as well as water is there. But here it doesn't\n",
      "[02:56.920]:\treally look like a beach, there are trees and everyone is just submerged in and so on,\n",
      "[03:01.400]:\tright? So based on that reasoning that it's not a beach or a pool, we can like conclude\n",
      "[03:07.480]:\tlike that it's a flood, right? So this is how we humans reason about it. But deep learning\n",
      "[03:12.400]:\tdoesn't do that. It only does what it is told to do. It cannot extrapolate and find new\n",
      "[03:19.480]:\tpatterns or find new information, just reason about something new that it was not trained\n",
      "[03:25.280]:\ton unless we did not it too.\n"
     ]
    }
   ],
   "source": [
    "# print the transcript\n",
    "transcript_str = []\n",
    "for segment in transcript:\n",
    "    transcript_str.append(f\"[{segment[0]}]:\\t{segment[1]}\")\n",
    "transcript_str = \"\\n\".join(transcript_str)\n",
    "print(transcript_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_start_times = [segment['start'] for segment in data['segments']]\n",
    "all_end_times = [segment['end'] for segment in data['segments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_time(time, all_times):\n",
    "    closest_time = min(all_times, key=lambda x: abs(x - time))\n",
    "    return closest_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_chunks = []\n",
    "for seek in range(0, int(all_end_times[-1]), 45):\n",
    "    chunk = {'start': None, 'end': None, 'text': None}\n",
    "\n",
    "    start_index = all_start_times.index(find_closest_time(seek, all_start_times))\n",
    "    chunk['start'] = all_start_times[start_index]\n",
    "    end_index = all_end_times.index(find_closest_time(seek + 60, all_end_times))\n",
    "    chunk['end'] = all_end_times[end_index]\n",
    "\n",
    "    chunk['text'] = \"\".join([segment['text'] for segment in data['segments'][start_index:end_index+1]]).strip()\n",
    "\n",
    "    transcript_chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0.0,\n",
       "  'end': 70.75999999999999,\n",
       "  'text': \"So, hello everybody. Welcome to the final module of this course on Neuro-Symbolic AI. So in the past few, like in the past, we have like, in the previous session, we looked at symbolic AI and we looked at basically, propositional logic, first order logic and program synthesis. So the program synthesis part is going to be something that we are going to use today. And we also looked at relation networks last time, although it was just like an overview of that. So today we will conclude by learning about neuro-symbolic AI. So let's get started here. Okay. So the contents for this session are going to be basically, we'll start of course\"},\n",
       " {'start': 46.8,\n",
       "  'end': 105.12,\n",
       "  'text': \"here. Okay. So the contents for this session are going to be basically, we'll start of course by talking about the problem. We'll again have a very quick review of symbolic AI, although we just looked at, but just a very quick review of symbolic AI from last time. And we also have like a difference between neural networks and symbolic AI. And then we look at the sort of flavor data set again, but a bit of a modified version that I created for neuro-symbolic AI. So we'll look at that and then we move on to the actual architecture and all the modules and everything that we'll be utilizing. So the entire work will start from here.\"},\n",
       " {'start': 89.32000000000001,\n",
       "  'end': 150.88,\n",
       "  'text': \"of flavor data set again, but a bit of a modified version that I created for neuro-symbolic AI. So we'll look at that and then we move on to the actual architecture and all the modules and everything that we'll be utilizing. So the entire work will start from here. So I guess we had already looked at this particular problem a while back, I think in one of the first few modules, I guess. So basically what we have here is basically an image of a flood, right? So if we give a deep learning model, which was not trained on floods or anything, it would obviously, a deep learning model would only like predict a few labels, I guess. So maybe like water and person. So it might say there's water, okay. So the deep learning model might just say water and there are people in it, so it will either say people or person. But apart from that, if we humans are given a definition of a flood, then we will actually\"},\n",
       " {'start': 133.2,\n",
       "  'end': 192.39999999999998,\n",
       "  'text': \"So maybe like water and person. So it might say there's water, okay. So the deep learning model might just say water and there are people in it, so it will either say people or person. But apart from that, if we humans are given a definition of a flood, then we will actually be able to reason about an image and get the flood answer, right? So as a human, what we do, we also see both water and people there, right? But we also see that there's water person, but also we see that it's not really a beach or anything, right? It's not like because even in a beach, people are there as well as water is there. But here it doesn't really look like a beach, there are trees and everyone is just submerged in and so on, right? So based on that reasoning that it's not a beach or a pool, we can like conclude like that it's a flood, right? So this is how we humans reason about it. But deep learning\"},\n",
       " {'start': 181.4,\n",
       "  'end': 218.4,\n",
       "  'text': \"right? So based on that reasoning that it's not a beach or a pool, we can like conclude like that it's a flood, right? So this is how we humans reason about it. But deep learning doesn't do that. It only does what it is told to do. It cannot extrapolate and find new patterns or find new information, just reason about something new that it was not trained on unless we did not it too.\"}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode query and documents\n",
    "chunk_texts = [chunk['text'] for chunk in transcript_chunks]\n",
    "doc_emb = model.encode(chunk_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = model.encode(\"give an example on the issue with deep learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [(chunk['start'], chunk['end'], chunk['text']) for chunk in transcript_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133.2 21.660459518432617 So maybe like water and person. So it might say there's water, okay. So the deep learning model might just say water and there are people in it, so it will either say people or person. But apart from that, if we humans are given a definition of a flood, then we will actually be able to reason about an image and get the flood answer, right? So as a human, what we do, we also see both water and people there, right? But we also see that there's water person, but also we see that it's not really a beach or anything, right? It's not like because even in a beach, people are there as well as water is there. But here it doesn't really look like a beach, there are trees and everyone is just submerged in and so on, right? So based on that reasoning that it's not a beach or a pool, we can like conclude like that it's a flood, right? So this is how we humans reason about it. But deep learning\n",
      "\n",
      "181.4 21.393245697021484 right? So based on that reasoning that it's not a beach or a pool, we can like conclude like that it's a flood, right? So this is how we humans reason about it. But deep learning doesn't do that. It only does what it is told to do. It cannot extrapolate and find new patterns or find new information, just reason about something new that it was not trained on unless we did not it too.\n",
      "\n",
      "89.32000000000001 18.951480865478516 of flavor data set again, but a bit of a modified version that I created for neuro-symbolic AI. So we'll look at that and then we move on to the actual architecture and all the modules and everything that we'll be utilizing. So the entire work will start from here. So I guess we had already looked at this particular problem a while back, I think in one of the first few modules, I guess. So basically what we have here is basically an image of a flood, right? So if we give a deep learning model, which was not trained on floods or anything, it would obviously, a deep learning model would only like predict a few labels, I guess. So maybe like water and person. So it might say there's water, okay. So the deep learning model might just say water and there are people in it, so it will either say people or person. But apart from that, if we humans are given a definition of a flood, then we will actually\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute dot score between query and all document embeddings\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "# Combine docs & scores\n",
    "chunk_score_tuples = [(*chunks[i], scores[i]) for i in range(len(chunks))]\n",
    "\n",
    "# Sort by decreasing score\n",
    "chunk_score_tuples = sorted(chunk_score_tuples, key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "# Output passages & scores\n",
    "top_k = 3\n",
    "for start, end, text, score in chunk_score_tuples[:top_k]:\n",
    "    if score > 16:\n",
    "        print(start, score, text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c34959fd061e0b75fe61b25789565ee0f65e3017c38ac9c89fb2c0e5483aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
