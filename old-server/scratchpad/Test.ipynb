{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa2dc9-949a-4550-be7e-6d87b159c5a6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65059c26-0850-4f2f-a40a-b0ea54c73ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1e6c74-1aac-49ff-8f8a-31fdca69e9cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript = [\n",
    "    {\n",
    "      \"time\": 0.0,\n",
    "      \"timestamp\": \"00:00.000\",\n",
    "      \"text\": \" so yeah so let's get started so let me just share my screen okay so I did the\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 24.22,\n",
    "      \"timestamp\": \"00:24.220\",\n",
    "      \"text\": \" toy neurosymbolic thing so I created a sample neurosymbolic program synthesis\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 31.439999999999998,\n",
    "      \"timestamp\": \"00:31.440\",\n",
    "      \"text\": \" engine so I used GPT-3 for that so there were a few observations that I made but\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 37.08,\n",
    "      \"timestamp\": \"00:37.080\",\n",
    "      \"text\": \" before that let me tell you what I did so I basically created a few prompt\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 43.879999999999995,\n",
    "      \"timestamp\": \"00:43.880\",\n",
    "      \"text\": \" example where I put in all the models so for each model I had a description I had\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 50.72,\n",
    "      \"timestamp\": \"00:50.720\",\n",
    "      \"text\": \" the arguments and the data types and the return what it returns and I also had a\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 59.08,\n",
    "      \"timestamp\": \"00:59.080\",\n",
    "      \"text\": \" few problem statements couple problem statement as examples with their\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 62.64,\n",
    "      \"timestamp\": \"01:02.640\",\n",
    "      \"text\": \" solution workflows so that was my prompting so a few observations that I\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 67.08,\n",
    "      \"timestamp\": \"01:07.080\",\n",
    "      \"text\": \" made were prompting with examples of feedback and corrections made the final\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 71.52,\n",
    "      \"timestamp\": \"01:11.520\",\n",
    "      \"text\": \" up would be more correct in one shot without requiring further changes so I\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 75.12,\n",
    "      \"timestamp\": \"01:15.120\",\n",
    "      \"text\": \" also did a feedback loop kind of situation where I used another instance\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 80.82000000000001,\n",
    "      \"timestamp\": \"01:20.820\",\n",
    "      \"text\": \" of GPT-3 which would give feedback on what could be changed what would be\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 84.48,\n",
    "      \"timestamp\": \"01:24.480\",\n",
    "      \"text\": \" improved what are the issues and that sort of thing and I also yeah so that\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 93.04,\n",
    "      \"timestamp\": \"01:33.040\",\n",
    "      \"text\": \" kind of so although I would recommend that so my suggestion is to or like my\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 99.4,\n",
    "      \"timestamp\": \"01:39.400\",\n",
    "      \"text\": \" approach or my cost-effective approach would be to rather fine-tune a critic\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 104.2,\n",
    "      \"timestamp\": \"01:44.200\",\n",
    "      \"text\": \" feedback model and use that for critic because critic is would be the most\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 109.56,\n",
    "      \"timestamp\": \"01:49.560\",\n",
    "      \"text\": \" important part because if that fails so we cannot rely on a few short prompted\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 113.48,\n",
    "      \"timestamp\": \"01:53.480\",\n",
    "      \"text\": \" GPT-3 critic instead I would put my bets on a fine-tuned version and also based\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 121.36,\n",
    "      \"timestamp\": \"02:01.360\",\n",
    "      \"text\": \" on the paper that you gave me that is the memory assisted prompt editing to\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 124.92,\n",
    "      \"timestamp\": \"02:04.920\",\n",
    "      \"text\": \" improve GPT-3 so what we could do is we could maintain a memory of all problem\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 130.52,\n",
    "      \"timestamp\": \"02:10.520\",\n",
    "      \"text\": \" statements and use document-based similarity matching to retrieve similar\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 136.88000000000002,\n",
    "      \"timestamp\": \"02:16.880\",\n",
    "      \"text\": \" problem statements and get those additional feedbacks and all that have\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 140.36,\n",
    "      \"timestamp\": \"02:20.360\",\n",
    "      \"text\": \" been given to those workflow outputs so this gives the model more context and\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 145.44,\n",
    "      \"timestamp\": \"02:25.440\",\n",
    "      \"text\": \" creates mechanism to use its knowledge from memory and improve efficiency\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 149.0,\n",
    "      \"timestamp\": \"02:29.000\",\n",
    "      \"text\": \" selectively so the fine so the items at hand or the action items that we have at\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 157.48,\n",
    "      \"timestamp\": \"02:37.480\",\n",
    "      \"text\": \" our hand would be yeah so the next step that we could do is we could work on\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 166.6,\n",
    "      \"timestamp\": \"02:46.600\",\n",
    "      \"text\": \" this memory bank solution for the program synthesis part the next would be\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 172.72,\n",
    "      \"timestamp\": \"02:52.720\",\n",
    "      \"text\": \" creating a model catalog the next task would be creating a logical planner and\"\n",
    "    },\n",
    "    {\n",
    "      \"time\": 178.68,\n",
    "      \"timestamp\": \"02:58.680\",\n",
    "      \"text\": \" the other task would be creating a pipeline executor for the same\"\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba27140-24d9-4c74-989c-6f9ec8fd8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"kYkpT5S1o6ajlTsFX6Dl2KElV9zqxJA5jxCNyFZ2\"\n",
    "\n",
    "headers = {\n",
    "  \"x-api-key\": API_KEY,\n",
    "  \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420f77f2-0a13-435b-8e83-565cfe62be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'transcript': transcript,\n",
    "    'query': \"what's the most cost effective approach?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47b83561-82d4-4e8b-8cc2-99226cbb48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_endpoint = \"http://localhost:9000/2015-03-31/functions/function/invocations\"\n",
    "base_url = \"https://qiyvjvwm57flqmhhawjq4t4y3u0djofs.lambda-url.us-east-1.on.aws\"\n",
    "hosted_endpoint = base_url + \"/gradientfire/transcript-search\"\n",
    "\n",
    "gpt_3_endpoint = \"https://cn7lpguukasl5z6wnvt7m6p2mi0lbmzm.lambda-url.us-east-1.on.aws/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10fe696b-2620-49ba-a35a-6b01fd8ec5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': [{'confidence': 17.40780258178711, 'time': 99.4, 'text': 'approach or my cost-effective approach would be to rather fine-tune a critic feedback model and use that for critic because critic is would be the most', 'timestamp': '01:39.400'}, {'confidence': 16.765840530395508, 'time': 75.12, 'text': 'also did a feedback loop kind of situation where I used another instance of GPT-3 which would give feedback on what could be changed what would be', 'timestamp': '01:15.120'}, {'confidence': 16.372028350830078, 'time': 93.04, 'text': 'kind of so although I would recommend that so my suggestion is to or like my approach or my cost-effective approach would be to rather fine-tune a critic', 'timestamp': '01:33.040'}], 'invocation_id': '719e22c3-2da0-44e8-9b7c-ce3c6759295a'}\n",
      "CPU times: user 10.2 ms, sys: 3.41 ms, total: 13.6 ms\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Send the POST request to the local endpoint\n",
    "response = requests.post(hosted_endpoint, headers=headers, json=payload).json()\n",
    "print(response)\n",
    "# if response.get('statusCode') == 200:\n",
    "#     output = response['body']['output']\n",
    "# else:\n",
    "#     output = response\n",
    "\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752b2a9-ac53-412c-9ae5-25bfd9a88440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd53abf3802f9ac4b10866aeeb92a3a9c2a4dc1eedeb5585da5af4dafe88d08e"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
