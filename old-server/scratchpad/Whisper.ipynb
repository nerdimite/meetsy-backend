{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"medium\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:11.240]  So, hello everybody. Welcome to the final module of this course on Neuro-Symbolic AI.\n",
      "[00:11.240 --> 00:20.400]  So in the past few, like in the past, we have like, in the previous session, we looked at\n",
      "[00:20.400 --> 00:26.260]  symbolic AI and we looked at basically, propositional logic, first order logic and program synthesis.\n",
      "[00:26.260 --> 00:30.760]  So the program synthesis part is going to be something that we are going to use today.\n",
      "[00:30.760 --> 00:35.520]  And we also looked at relation networks last time, although it was just like an overview\n",
      "[00:35.520 --> 00:46.800]  of that. So today we will conclude by learning about neuro-symbolic AI. So let's get started\n",
      "[00:46.800 --> 00:47.800]  here.\n",
      "[00:47.800 --> 01:10.760]  Okay. So the contents for this session are going to be basically, we'll start of course\n",
      "[01:10.760 --> 01:16.120]  by talking about the problem. We'll again have a very quick review of symbolic AI, although\n",
      "[01:16.120 --> 01:22.360]  we just looked at, but just a very quick review of symbolic AI from last time. And we also\n",
      "[01:22.360 --> 01:29.320]  have like a difference between neural networks and symbolic AI. And then we look at the sort\n",
      "[01:29.320 --> 01:34.040]  of flavor data set again, but a bit of a modified version that I created for neuro-symbolic\n",
      "[01:34.040 --> 01:39.400]  AI. So we'll look at that and then we move on to the actual architecture and all the\n",
      "[01:39.400 --> 01:45.120]  modules and everything that we'll be utilizing. So the entire work will start from here.\n",
      "[01:45.120 --> 01:55.080]  So I guess we had already looked at this particular problem a while back, I think in one of the\n",
      "[01:55.080 --> 02:01.960]  first few modules, I guess. So basically what we have here is basically an image of a flood,\n",
      "[02:01.960 --> 02:06.840]  right? So if we give a deep learning model, which was not trained on floods or anything,\n",
      "[02:06.840 --> 02:13.200]  it would obviously, a deep learning model would only like predict a few labels, I guess.\n",
      "[02:13.200 --> 02:17.680]  So maybe like water and person. So it might say there's water, okay. So the deep learning\n",
      "[02:17.680 --> 02:25.320]  model might just say water and there are people in it, so it will either say people or person.\n",
      "[02:25.320 --> 02:30.880]  But apart from that, if we humans are given a definition of a flood, then we will actually\n",
      "[02:30.880 --> 02:37.520]  be able to reason about an image and get the flood answer, right? So as a human, what we\n",
      "[02:37.520 --> 02:48.200]  do, we also see both water and people there, right? But we also see that there's water\n",
      "[02:48.200 --> 02:52.600]  person, but also we see that it's not really a beach or anything, right? It's not like\n",
      "[02:52.600 --> 02:56.920]  because even in a beach, people are there as well as water is there. But here it doesn't\n",
      "[02:56.920 --> 03:01.400]  really look like a beach, there are trees and everyone is just submerged in and so on,\n",
      "[03:01.400 --> 03:07.480]  right? So based on that reasoning that it's not a beach or a pool, we can like conclude\n",
      "[03:07.480 --> 03:12.400]  like that it's a flood, right? So this is how we humans reason about it. But deep learning\n",
      "[03:12.400 --> 03:19.480]  doesn't do that. It only does what it is told to do. It cannot extrapolate and find new\n",
      "[03:19.480 --> 03:25.280]  patterns or find new information, just reason about something new that it was not trained\n",
      "[03:25.280 --> 03:38.800]  on unless we did not it too.\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(\"Recording.m4a\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" So, hello everybody. Welcome to the final module of this course on Neuro-Symbolic AI. So in the past few, like in the past, we have like, in the previous session, we looked at symbolic AI and we looked at basically, propositional logic, first order logic and program synthesis. So the program synthesis part is going to be something that we are going to use today. And we also looked at relation networks last time, although it was just like an overview of that. So today we will conclude by learning about neuro-symbolic AI. So let's get started here. Okay. So the contents for this session are going to be basically, we'll start of course by talking about the problem. We'll again have a very quick review of symbolic AI, although we just looked at, but just a very quick review of symbolic AI from last time. And we also have like a difference between neural networks and symbolic AI. And then we look at the sort of flavor data set again, but a bit of a modified version that I created for neuro-symbolic AI. So we'll look at that and then we move on to the actual architecture and all the modules and everything that we'll be utilizing. So the entire work will start from here. So I guess we had already looked at this particular problem a while back, I think in one of the first few modules, I guess. So basically what we have here is basically an image of a flood, right? So if we give a deep learning model, which was not trained on floods or anything, it would obviously, a deep learning model would only like predict a few labels, I guess. So maybe like water and person. So it might say there's water, okay. So the deep learning model might just say water and there are people in it, so it will either say people or person. But apart from that, if we humans are given a definition of a flood, then we will actually be able to reason about an image and get the flood answer, right? So as a human, what we do, we also see both water and people there, right? But we also see that there's water person, but also we see that it's not really a beach or anything, right? It's not like because even in a beach, people are there as well as water is there. But here it doesn't really look like a beach, there are trees and everyone is just submerged in and so on, right? So based on that reasoning that it's not a beach or a pool, we can like conclude like that it's a flood, right? So this is how we humans reason about it. But deep learning doesn't do that. It only does what it is told to do. It cannot extrapolate and find new patterns or find new information, just reason about something new that it was not trained on unless we did not it too.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c34959fd061e0b75fe61b25789565ee0f65e3017c38ac9c89fb2c0e5483aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
