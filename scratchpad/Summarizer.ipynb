{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-3 Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from whisper.utils import format_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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' # os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript = \"\"\"\n",
    "# [00:00:00]:\tWithout further ado, let’s bring out the Steve I think you all are here to see.\n",
    "\n",
    "# [00:00:10]: \tSteve Jobs.\n",
    "\n",
    "# [00:00:24]:\tJust this week you surpassed Microsoft in, um, market valuation, and we thought you might have a thought or two about that.\n",
    "\n",
    "# [00:00:35]: \tUm… it… for those of us that have been in the industry a long time, it’s surreal. Uh… but, it… it doesn’t matter very much.\n",
    "\n",
    "# [00:00:45]:\tSo, you published a long, open letter. Something like, what did you call it? Thoughts on Flash or something like that. So, you’ve… you’ve clearly, uh, stated your case there. Is it really fair or the best thing for consumers who buy… say an iPad or an iPhone to just be abrupt?\n",
    "\n",
    "# [00:01:05]:\tApple is a company that has… doesn’t have the most resources of everybody in the world. And, um, the way we’ve succeeded is by choosing what horses to ride really carefully, technically. We try to look for these technical vectors that have a future. And that are headed up. And, you know, technology… different pieces of technology kind of go in cycles. They have their, their springs and summers and autumns and then they, you know, go to the graveyard of technology. And so we try to pick things that are in their springs. And if you choose wisely, you can save yourself an enormous amount of work versus trying to do everything. So, we have a history of doing that. As an example, we went from the five inch floppy disk to three and a half inch floppy disk with the Mac. And sometimes when we get rid of things, like the [00:02:00] floppy disk drive on the original iMac, people call us crazy. But, sometimes you just have to pick the things that look like they’re going to be the right horses to ride going forward. And… and flash looks like a technology that had its day. But is really on the… is waning. And HTML 5 looks like the technology that’s really on the ascendancy right now.\n",
    "\n",
    "# [00:02:20]:\tIt’s also a development environment and there are entire websites… some of them quite beautiful… written on a flash platform.\n",
    "\n",
    "# [00:02:28]:\tYou know, one of the… an even more popular development environment was HyperCard, and we were okay to axe that. It wasn’t more popular than Flash, was it?\n",
    "\n",
    "# [00:02:38]:\tOh… in its day, sure it was.\n",
    "\n",
    "# [00:02:40]:\tOn your platform, right?\n",
    "\n",
    "# [00:04:35]: Hey Lisa, add a joke about the iPad being slow in the action items.\n",
    "\n",
    "# [00:04:45]: Also add reinstalling flash to the list.\n",
    "\n",
    "# [00:02:43]:\tNo, no, no, no, no. HyperCard was huge in its day because it was… because it was accessible to anybody. We… we didn’t start off to have a war with Flash or anything else. We just made a technical decision that we weren’t going to put the energy into getting Flash on our platform. We told Adobe, “If you ever have this thing running fast, come back and show us.” Which they never did. And… uh… but… but people were not going to use it. And that was it. And we shipped the iPhone. And it doesn’t use Flash. And it wasn’t until we shipped the iPad, and it didn’t use flash, that Adobe started to raise a stink about it. And that’s why I wrote Thoughts on Flash… was because we were trying to be real professional about this and weren’t talking to the press about it. We didn’t think it was a matter for the press. And we finally just said, enough is enough. We’re tired of these guys trashing us in the press over this.\n",
    "\n",
    "# [00:03:15]:\tWhat if the market says, “Hey, you know, it’s important enough to us to be able to run not just these videos but whole websites.” What if people say, you know, the iPad is… uh… crippled in this respect?\n",
    "\n",
    "# [00:03:25]:\tWell, you know, well I’d say two things. Number one, things are packages of… of emphasis. Some things are emphasized in a product. Some things are not done as well in a product. Some things are chosen not to be done at all in a product. And so different people make [00:04:00] different choices. And, uh, if the market tells we’re making the wrong choices, we listen to the market. We’re just… we’re just people running this company. We’re trying to make great products for people. And so we have at least the courage of our convictions to say we don’t think this is part of what makes a great product. We’re going to leave it out. That’s what a lot of customers pay us to do… is to try to make the products that we can. And if we succeed, they’ll buy them. And if we don’t, they won’t. And it’ll all work itself out.\n",
    "\n",
    "# [00:04:10]:\tSo, uh.\n",
    "\n",
    "# [00:04:12]: \tSo, you know so far, I’d have to say, that that that people seem to be liking iPads. You know. I mean we’ve sold one every three seconds since launched it. So, I don’t know how it’s going to come out.\n",
    "# 2\n",
    "\n",
    "# [00:04:30]:\tIt takes you three whole seconds to sell one? Can’t you do better?\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.json') as f:\n",
    "    data = json.load(f)\n",
    "    full_text = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" So, hello everybody. Welcome to the final module of this course on Neuro-Symbolic AI. So in the past few, like in the past, we have like, in the previous session, we looked at symbolic AI and we looked at basically, propositional logic, first order logic and program synthesis. So the program synthesis part is going to be something that we are going to use today. And we also looked at relation networks last time, although it was just like an overview of that. So today we will conclude by learning about neuro-symbolic AI. So let's get started here. Okay. So the contents for this session are going to be basically, we'll start of course by talking about the problem. We'll again have a very quick review of symbolic AI, although we just looked at, but just a very quick review of symbolic AI from last time. And we also have like a difference between neural networks and symbolic AI. And then we look at the sort of flavor data set again, but a bit of a modified version that I created for neuro-symbolic AI. So we'll look at that and then we move on to the actual architecture and all the modules and everything that we'll be utilizing. So the entire work will start from here. So I guess we had already looked at this particular problem a while back, I think in one of the first few modules, I guess. So basically what we have here is basically an image of a flood, right? So if we give a deep learning model, which was not trained on floods or anything, it would obviously, a deep learning model would only like predict a few labels, I guess. So maybe like water and person. So it might say there's water, okay. So the deep learning model might just say water and there are people in it, so it will either say people or person. But apart from that, if we humans are given a definition of a flood, then we will actually be able to reason about an image and get the flood answer, right? So as a human, what we do, we also see both water and people there, right? But we also see that there's water person, but also we see that it's not really a beach or anything, right? It's not like because even in a beach, people are there as well as water is there. But here it doesn't really look like a beach, there are trees and everyone is just submerged in and so on, right? So based on that reasoning that it's not a beach or a pool, we can like conclude like that it's a flood, right? So this is how we humans reason about it. But deep learning doesn't do that. It only does what it is told to do. It cannot extrapolate and find new patterns or find new information, just reason about something new that it was not trained on unless we did not it too.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transcript\n",
    "transcript = []\n",
    "for segment in data['segments']:\n",
    "    transcript.append((format_timestamp(segment['start']), segment['text'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000]:\tSo, hello everybody. Welcome to the final module of this course on Neuro-Symbolic AI.\n",
      "[00:11.240]:\tSo in the past few, like in the past, we have like, in the previous session, we looked at\n",
      "[00:20.400]:\tsymbolic AI and we looked at basically, propositional logic, first order logic and program synthesis.\n",
      "[00:26.260]:\tSo the program synthesis part is going to be something that we are going to use today.\n",
      "[00:30.760]:\tAnd we also looked at relation networks last time, although it was just like an overview\n",
      "[00:35.520]:\tof that. So today we will conclude by learning about neuro-symbolic AI. So let's get started\n",
      "[00:46.800]:\there.\n",
      "[00:47.800]:\tOkay. So the contents for this session are going to be basically, we'll start of course\n",
      "[01:10.760]:\tby talking about the problem. We'll again have a very quick review of symbolic AI, although\n",
      "[01:16.120]:\twe just looked at, but just a very quick review of symbolic AI from last time. And we also\n",
      "[01:22.360]:\thave like a difference between neural networks and symbolic AI. And then we look at the sort\n",
      "[01:29.320]:\tof flavor data set again, but a bit of a modified version that I created for neuro-symbolic\n",
      "[01:34.040]:\tAI. So we'll look at that and then we move on to the actual architecture and all the\n",
      "[01:39.400]:\tmodules and everything that we'll be utilizing. So the entire work will start from here.\n",
      "[01:45.120]:\tSo I guess we had already looked at this particular problem a while back, I think in one of the\n",
      "[01:55.080]:\tfirst few modules, I guess. So basically what we have here is basically an image of a flood,\n",
      "[02:01.960]:\tright? So if we give a deep learning model, which was not trained on floods or anything,\n",
      "[02:06.840]:\tit would obviously, a deep learning model would only like predict a few labels, I guess.\n",
      "[02:13.200]:\tSo maybe like water and person. So it might say there's water, okay. So the deep learning\n",
      "[02:17.680]:\tmodel might just say water and there are people in it, so it will either say people or person.\n",
      "[02:25.320]:\tBut apart from that, if we humans are given a definition of a flood, then we will actually\n",
      "[02:30.880]:\tbe able to reason about an image and get the flood answer, right? So as a human, what we\n",
      "[02:37.520]:\tdo, we also see both water and people there, right? But we also see that there's water\n",
      "[02:48.200]:\tperson, but also we see that it's not really a beach or anything, right? It's not like\n",
      "[02:52.600]:\tbecause even in a beach, people are there as well as water is there. But here it doesn't\n",
      "[02:56.920]:\treally look like a beach, there are trees and everyone is just submerged in and so on,\n",
      "[03:01.400]:\tright? So based on that reasoning that it's not a beach or a pool, we can like conclude\n",
      "[03:07.480]:\tlike that it's a flood, right? So this is how we humans reason about it. But deep learning\n",
      "[03:12.400]:\tdoesn't do that. It only does what it is told to do. It cannot extrapolate and find new\n",
      "[03:19.480]:\tpatterns or find new information, just reason about something new that it was not trained\n",
      "[03:25.280]:\ton unless we did not it too.\n"
     ]
    }
   ],
   "source": [
    "# print the transcript\n",
    "transcript_str = []\n",
    "for segment in transcript:\n",
    "    transcript_str.append(f\"[{segment[0]}]:\\t{segment[1]}\")\n",
    "transcript_str = \"\\n\".join(transcript_str)\n",
    "print(transcript_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_prompt = f\"\"\"Generate the minutes of the meeting for the following transcript:\n",
    "Meeting Transcription:\n",
    "{transcript_str}\n",
    "\n",
    "Meeting Minutes:\n",
    "-\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt3(prompt, max_tokens=256, temperature=0.5, top_p=1, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty\n",
    "    )\n",
    "    return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-The final module of the course on Neuro-Symbolic AI was discussed.\n",
      "-The contents for the session were reviewed.\n",
      "-The problem was looked at again and discussed in depth.\n",
      "-The differences between neural networks and symbolic AI were explored.\n",
      "-The work for the session was started.\n"
     ]
    }
   ],
   "source": [
    "raw_minutes = '\\n-' + run_gpt3(mom_prompt, temperature=0.5)\n",
    "print(raw_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(raw_minutes):\n",
    "    minutes = raw_minutes.split('\\n-')\n",
    "    minutes = [minute.strip() for minute in minutes]\n",
    "    minutes = [minute for minute in minutes if minute != '']\n",
    "    return minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_minutes(minutes):\n",
    "    for minute in minutes:\n",
    "        print(f'-> {minute}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> The final module of the course on Neuro-Symbolic AI was discussed.\n",
      "-> The contents for the session were reviewed.\n",
      "-> The problem was looked at again and discussed in depth.\n",
      "-> The differences between neural networks and symbolic AI were explored.\n",
      "-> The work for the session was started.\n"
     ]
    }
   ],
   "source": [
    "print_minutes(post_process(raw_minutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Item Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_prompt = f\"\"\"Extract the Action Items / To-Do List from the Transcript.\n",
    "Meeting Transcription:\n",
    "{transcript_str}\n",
    "\n",
    "Action Items:\n",
    "-\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_action_items = run_gpt3(action_prompt, temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Review symbolic AI from last time\n",
      "-> Look at the modified version of the flavor data set\n",
      "-> Learn about the architecture and modules for neuro-symbolic AI\n"
     ]
    }
   ],
   "source": [
    "print_minutes(post_process(raw_action_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bart Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"knkarthick/MEETING_SUMMARY\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"knkarthick/MEETING_SUMMARY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"knkarthick/MEETING_SUMMARY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes = summarizer(transcript_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the final module of the course on Neuro-Symbolic AI, we will review symbolic AI, propositional logic, first order logic and program synthesis\n",
      " We will also look at relation networks and deep learning\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(minutes[0]['summary_text'].split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c34959fd061e0b75fe61b25789565ee0f65e3017c38ac9c89fb2c0e5483aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
